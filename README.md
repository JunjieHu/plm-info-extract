# Pre-train LM for Information Extraction

## Meeting Note:
- 06/13/2022


## Related Papers
- Mintz et al. ACL'09. [Distant supervision for relation extraction without labeled data](https://aclanthology.org/P09-1113.pdf)
- Riedel et al. ECML PKDD'10. [Modeling Relations and Their Mentions without
Labeled Text](https://link.springer.com/content/pdf/10.1007/978-3-642-15939-8_10.pdf)
- Thillaisundaram et al. BioNLP'19. [Biomedical relation extraction with pre-trained language representations
and minimal task-specific architecture](https://aclanthology.org/D19-5713.pdf)
- Verga, et al. NAACL'18. [Simultaneously Self-Attending to All Mentions for
Full-Abstract Biological Relation Extraction](https://aclanthology.org/N18-1080.pdf)
- Ye et al. NAACL'19. [Distant Supervision Relation Extraction with Intra-Bag
and Inter-Bag Attentions](https://aclanthology.org/N19-1288.pdf)
- Alt et al. ACL'19. [Fine-tuning Pre-Trained Transformer Language Models to Distantly
Supervised Relation Extraction](https://aclanthology.org/P19-1134.pdf)
- Wang et al. ACL'22. [OIE@OIA: an Adaptable and Efficient Open Information Extraction
Framework](https://aclanthology.org/2022.acl-long.430.pdf)
